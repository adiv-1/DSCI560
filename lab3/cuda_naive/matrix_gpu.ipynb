{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050894cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matrixMultiplyGPU(float *A, float *B, float *C, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    int N = 512;\n",
    "    if (argc > 1) {\n",
    "        N = atoi(argv[1]);\n",
    "    }\n",
    "\n",
    "    size_t matrix_size = N * N * sizeof(float);\n",
    "    \n",
    "    float *host_A = (float *)malloc(matrix_size);\n",
    "    float *host_B = (float *)malloc(matrix_size);\n",
    "    float *host_C = (float *)malloc(matrix_size);\n",
    "    \n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        host_A[i] = rand() % 100 / 100.0f;\n",
    "        host_B[i] = rand() % 100 / 100.0f;\n",
    "    }    \n",
    "    float *device_A;\n",
    "    float *device_B;\n",
    "    float *device_C;\n",
    "    \n",
    "    cudaMalloc((void**)&device_A, matrix_size);\n",
    "    cudaMalloc((void**)&device_B, matrix_size);\n",
    "    cudaMalloc((void**)&device_C, matrix_size);\n",
    "    \n",
    "    cudaMemcpy(device_A, host_A, matrix_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(device_B, host_B, matrix_size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    \n",
    "    // Calculate how many blocks we need\n",
    "    int num_blocks = (N + 15) / 16;\n",
    "    dim3 numBlocks(num_blocks, num_blocks);\n",
    "    \n",
    "    printf(\"Using %d x %d blocks with 16 x 16 threads per block\\n\", num_blocks, num_blocks);\n",
    "    \n",
    "    cudaEvent_t start_event, stop_event;\n",
    "    cudaEventCreate(&start_event);\n",
    "    cudaEventCreate(&stop_event);\n",
    "    \n",
    "    //start time\n",
    "    cudaEventRecord(start_event);    \n",
    "    matrixMultiplyGPU<<<numBlocks, threadsPerBlock>>>(device_A, device_B, device_C, N);\n",
    "    cudaEventRecord(stop_event); //stop time\n",
    "    cudaEventSynchronize(stop_event);\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start_event, stop_event);\n",
    "    \n",
    "    printf(\"Naive CUDA execution time (N=%d): %f ms\\n\", N, milliseconds);\n",
    "    printf(\"In %f seconds\\n\", milliseconds / 1000.0f);\n",
    "    \n",
    "    cudaMemcpy(host_C, device_C, matrix_size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    volatile float prevent_optimization = host_C[0];\n",
    "    \n",
    "    cudaFree(device_A);\n",
    "    cudaFree(device_B);\n",
    "    cudaFree(device_C);\n",
    "    \n",
    "    free(host_A);\n",
    "    free(host_B);\n",
    "    free(host_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a3b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc matrix_gpu.cu -o matrix_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01966c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./matrix_gpu 512\n",
    "!./matrix_gpu 1024\n",
    "!./matrix_gpu 2048"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
